#Статистика 

Пусть $X$ и $Y$ — [[Дискретная случайная величина|дискретные случайные величины]], а $f$ — функция. Тогда [[Математическое ожидание|математическое ожидание]] $f(X, Y)$ равно

$$E[f(X, Y)]=\sum_{x}​\sum_{y}​f(x, y)⋅p_X,_Y​(x, y)$$

А если случайные величины [[Независимость случайных величин|независимы]], то даже не нужно знать [[Функция распределения для двух дискретных случайных величин|совместное распределение]]:

$$E[f(X, Y)]=\sum_{x}\sum_{y}​f(x, y)⋅p_X​(x)⋅p_Y​(y)$$

Математическое ожидание суммы случайных величин можно представить как сумму их математических ожиданий.

В общем случае для любых случайных величин $X$ и $Y$ и чисел $a, b, c$ верно следующее:

$$E[aX+bY+c]=aE[X]+bE[Y]+c$$

Это свойство удобно тем, что оно работает как для зависимых, так и для независимых случайных величин

$$Var[aX+b]=a^2Var[X]$$

для случайной величины $X$ и произвольных чисел $a, b$.